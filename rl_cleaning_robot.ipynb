{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e208554-5823-4e0c-96d5-05e2fe17d41a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy evaluation state values:\n",
      "{'s1': -0.9399999999999995, 's2': -0.9999999999999994, 's3': -0.8859999999999996, 's4': -0.9399999999999995}\n",
      "\n",
      "Value iteration optimal state values:\n",
      "{'s1': 0.5720000000000001, 's2': 0.68, 's3': 0.68, 's4': 0.8}\n",
      "\n",
      "Stochastic environment value iteration optimal state values:\n",
      "{'s1': 0.536323299872, 's2': 0.6403592220800001, 's3': 0.6403592220800001, 's4': 0.7560718444160002}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Problem description: The robot aims to clean the dusts in room D.\n",
    "# The layout of the rooms are shown below.\n",
    "# --------------\n",
    "# |  A  |  B   |\n",
    "# --------------\n",
    "# |  C  | .D.  |\n",
    "# --------------\n",
    "\n",
    "# Define states\n",
    "states = [\n",
    "    's1',  # at A, D uncleaned\n",
    "    's2',  # at B, D uncleaned\n",
    "    's3',  # at C, D uncleaned\n",
    "    's4',  # at D, D uncleaned\n",
    "    's5'   # at D, D cleaned\n",
    "]\n",
    "\n",
    "# Terminal state is s5, where D is cleaned\n",
    "terminal_states = ['s5']\n",
    "\n",
    "# Define actions\n",
    "actions = ['left', 'right', 'up', 'down', 'suck']\n",
    "\n",
    "# Applicable actions for each state, except terminal states\n",
    "applicable_actions = [\n",
    "    ['right', 'down', 'suck'],  # for state s1\n",
    "    ['left', 'down', 'suck'],  # for state s2\n",
    "    ['right', 'up', 'suck'],  # for state s3\n",
    "    ['left', 'up', 'suck']  # for state s4\n",
    "]\n",
    "\n",
    "# Next state when applying each applicable action in each state\n",
    "next_states = [\n",
    "    ['s2', 's3', 's1'],   # s'(s1, a)\n",
    "    ['s1', 's4', 's2'],   # s'(s2, a)\n",
    "    ['s4', 's1', 's3'],   # s'(s3, a)\n",
    "    ['s3', 's2', 's5']   # s'(s4, a)\n",
    "]\n",
    "\n",
    "# Reward when applying each applicable action in each state\n",
    "rewards = [\n",
    "    [-0.04, -0.04, -0.1],   # r(s1, a)\n",
    "    [-0.04, -0.04, -0.1],   # r(s2, a)\n",
    "    [-0.04, -0.04, -0.1],   # r(s3, a)\n",
    "    [-0.04, -0.04, -0.1]   # r(s4, a)\n",
    "]\n",
    "\n",
    "# Discount factor\n",
    "gamma = 0.9\n",
    "\n",
    "# When reaching the terminal state s5, we receive a reward of 1.0.\n",
    "terminal_reward = 1.0\n",
    "\n",
    "###\n",
    "# Policy evaluation\n",
    "###\n",
    "\n",
    "\"\"\"\n",
    "This policy evaluation method takes a policy \n",
    "and returns the state values for all the states under this policy, \n",
    "except the terminal states.\n",
    "\"\"\"\n",
    "def policy_evaluation(policy):\n",
    "    v = {} # initialise a dictionary 'v' to map states to state values\n",
    "    for state in states:\n",
    "        # foreach terminal state s_T, initialise its state value as its predefined reward v_π(s_T) = r_sT\n",
    "        if state in terminal_states: \n",
    "            v[state] = terminal_reward\n",
    "        # foreach state, set its state value v_π(s) to 0\n",
    "        else: \n",
    "            v[state] = 0\n",
    "            \n",
    "    # Iterate until convergence\n",
    "    while True:\n",
    "        delta = 0 # value gap ∆ = 0\n",
    "        # iterate over each state (except terminal states)\n",
    "        for i, state in enumerate(states[:-1]): # excluding last index, which is terminal state s5\n",
    "            v_old = v[state] # copying old state value to v_old\n",
    "            \n",
    "            # Updating state value: finding all the calculation components\n",
    "            action = policy[state] # Get the action for the state from the policy dictionary\n",
    "            action_index = applicable_actions[i].index(action) # find the index of that action in the applicable_actions list\n",
    "            next_state = next_states[i][action_index] # Get the next state for the current (state, action) pair\n",
    "            reward = rewards[i][action_index] # Get the reward for the current (state, action) pair\n",
    "\n",
    "            # Calculate the updated the state value: v_π(s) = r_(s,π(s)) + γ ∗ v_π(s′_(s,π(s)))\n",
    "            v[state] = reward + gamma * v[next_state]\n",
    "\n",
    "            if abs(v_old - v[state]) > delta: # Check if the value gap ∆ needs to be updated\n",
    "                delta = abs(v_old - v[state])\n",
    "\n",
    "        if delta == 0:\n",
    "            return dict(list(v.items())[:-1]) # return all states, except the terminal state\n",
    "\n",
    "pi = {\n",
    "        's1': 'right',\n",
    "        's2': 'suck',\n",
    "        's3': 'up',\n",
    "        's4': 'up'}\n",
    "\n",
    "state_values = policy_evaluation(pi)\n",
    "print(\"Policy evaluation state values:\")\n",
    "print(state_values)\n",
    "\n",
    "\n",
    "###\n",
    "# Q1.2: value iteration\n",
    "###\n",
    "\n",
    "\"\"\"\n",
    "This value iteration method returns the optimal state values for all the states,\n",
    "except the terminal states.\n",
    "\"\"\"\n",
    "def value_iteration():\n",
    "    v = {} # initialise a dictionary 'v' to map states to state values\n",
    "    for state in states:\n",
    "        # foreach terminal state s_T, initialise its state value as its predefined reward v_π(s_T) = r_sT\n",
    "        if state in terminal_states: \n",
    "            v[state] = terminal_reward\n",
    "        # foreach state, set its state value v_π(s) to 0\n",
    "        else: \n",
    "            v[state] = 0\n",
    "            \n",
    "    # Iterate until convergence\n",
    "    while True:\n",
    "        delta = 0 # value gap ∆ = 0\n",
    "        # iterate over each state (except terminal states)\n",
    "        for state in states[:-1]: # excluding last index, which is terminal state s5\n",
    "            v_old = v[state] # copying old state value to v_old\n",
    "\n",
    "            # Update state value\n",
    "            max_value = float('-inf') # Any value will be bigger than negative infinity\n",
    "            # Iterate through all applicable actions for the current state\n",
    "            for action_index, action in enumerate(applicable_actions[states.index(state)]):\n",
    "                # select the specific next state based on the current (state, action) pair being considered\n",
    "                next_state = next_states[states.index(state)][action_index] \n",
    "                # select the specific reward based on the current (state, action) pair being considered\n",
    "                reward = rewards[states.index(state)][action_index]\n",
    "\n",
    "                # Calculate the resulting value for this (state, action) pair based on the next_state and reward\n",
    "                resulting_value = reward + gamma * v[next_state]\n",
    "                max_value = max(max_value, resulting_value) # compare current max with resulting_value and retain the larger value\n",
    "\n",
    "            v[state] = max_value # update state value to the ultimate maximum value\n",
    "            if abs(v_old - v[state]) > delta: # update delta if necessary\n",
    "                delta = abs(v_old - v[state])\n",
    "\n",
    "        if delta == 0:\n",
    "            return dict(list(v.items())[:-1]) # return all states, except the terminal state\n",
    "\n",
    "# Run value iteration\n",
    "state_values = value_iteration()\n",
    "print(\"\\nValue iteration optimal state values:\")\n",
    "print(state_values)\n",
    "\n",
    "###\n",
    "# Stochastic environment value iteration\n",
    "###\n",
    "def stochastic_value_iteration():\n",
    "    v = {} # initialise a dictionary 'v' to map states to state values\n",
    "    for state in states:\n",
    "        # foreach terminal state s_T, initialise its state value as its predefined reward v_π(s_T) = r_sT\n",
    "        if state in terminal_states: \n",
    "            v[state] = terminal_reward\n",
    "        # foreach state, set its state value v_π(s) to 0\n",
    "        else: \n",
    "            v[state] = 0\n",
    "            \n",
    "    # Iterate until convergence\n",
    "    while True:\n",
    "        delta = 0 # value gap ∆ = 0\n",
    "        # iterate over each state (except terminal states)\n",
    "        for state in states[:-1]: # excluding last index, which is terminal state s5\n",
    "            v_old = v[state] # copying old state value to v_old\n",
    "\n",
    "            # Update state value\n",
    "            max_value = float('-inf') # Any value will be bigger than negative infinity\n",
    "            # Iterate through all applicable actions for the current state\n",
    "            for action_index, action in enumerate(applicable_actions[states.index(state)]):\n",
    "                # select the specific reward based on the current (state, action) pair being considered\n",
    "                reward = rewards[states.index(state)][action_index]\n",
    "                if (state == 's4' and action == 'suck'): \n",
    "                    success_state = 's5' # room is successfully cleaned\n",
    "                    failure_state = state # room remains in s4, so it remains unclean\n",
    "                    # 80% probability of success, 20% of failure\n",
    "                    resulting_value = (0.8 * (reward + gamma * v[success_state])) + (0.2 * (reward + gamma * v[failure_state]))\n",
    "                else:\n",
    "                    # select the specific next state based on the current (state, action) pair being considered\n",
    "                    next_state = next_states[states.index(state)][action_index] \n",
    "                    # Calculate the resulting value for this (state, action) pair based on the next_state and reward\n",
    "                    resulting_value = reward + gamma * v[next_state]\n",
    "                \n",
    "                max_value = max(max_value, resulting_value) # compare current max with resulting_value and retain the larger value\n",
    "            v[state] = max_value # update state value to the ultimate maximum value (after looping through every action)\n",
    "            if abs(v_old - v[state]) > delta: # update delta if necessary\n",
    "                delta = abs(v_old - v[state])\n",
    "                    \n",
    "            if delta < 0.001:\n",
    "                return dict(list(v.items())[:-1]) # return all states, except the terminal state\n",
    "\n",
    "# Run stochastic environment value iteration\n",
    "stochastic_state_values = stochastic_value_iteration()\n",
    "print(\"\\nStochastic environment value iteration optimal state values:\")\n",
    "print(stochastic_state_values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
